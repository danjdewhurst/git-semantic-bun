# v0.4 Performance Plan

Goal: improve end-to-end speed (cold start, warm query latency, large-repo scalability) without sacrificing result quality.

## Target metrics

Benchmarks should be tracked for:

- **Cold search latency** (process start + model load + search)
- **Warm search latency** (reused model/index)
- **Index load time**
- **Update time per 1k commits**
- **Memory footprint**

Initial targets:

- Warm search p50: **< 150ms** on ~50k commits
- Warm search p95: **< 300ms** on ~50k commits
- Cold search: **-40%** vs current baseline
- Index load: **-35%** on compact indexes

## Priority order

## P0 — Biggest wins

### 1) Daemon mode (`gsb serve`)

Keep model + index + lexical caches loaded and accept repeated queries over stdin.

Deliverables:

- `gsb serve` command
- line-based query protocol (stdin in, formatted results out)
- optional JSON protocol mode
- idle timeout + graceful shutdown

Expected impact: major reduction in repeated cold starts.

---

### 2) Lexical cache precomputation

Precompute and cache BM25 corpus statistics per index snapshot.

Deliverables:

- cached token stats keyed by index checksum
- avoid recomputing BM25 DF/tf scaffolding each query
- cache invalidation on index checksum change

Expected impact: lower CPU cost per query and more predictable latency.

---

### 3) Hot-path scoring and allocation optimisation

Reduce JS object churn in ranking path.

Deliverables:

- score buffers with typed arrays where practical
- reduced intermediate object creation in ranking/top-k
- avoid repeated string joins/tokenisation where not needed

Expected impact: lower GC pressure and better p95 latency.

## P1 — Large repo scalability

### 4) Memory-mapped/lazy vector loading

- Load only required vector chunks where possible.
- Keep compact metadata hot, vectors demand-driven.

### 5) Optional ANN backend

- Introduce optional ANN mode for very large indexes.
- Keep exact search as default for small/medium repos.

## P2 — Operational confidence

### 6) Perf CI gates and baseline snapshots

- Persist benchmark snapshots in CI artifacts
- guardrails for regression thresholds
- separate cold/warm benchmark suites

## Implementation slices

1. Add perf benchmark harness + baseline capture (done first)
2. Build daemon MVP
3. Add lexical cache keyed by checksum
4. Optimise hot scoring path
5. Add mmap/lazy vector prototype
6. Evaluate ANN plugin path

## Validation strategy

- Keep semantic relevance tests (golden ranking) unchanged and green.
- Require perf gains without quality regression.
- Compare before/after with stable synthetic and repository-backed datasets.
